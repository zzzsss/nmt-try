defaults:
  experiment:
    model_file: ./<EXP>.mod
    hyp_file: ./<EXP>.hyp
    out_file: ./<EXP>.out
    err_file: ./<EXP>.err
    run_for_epochs: ${max_epochs}
    eval_metrics: bleu
  train:
    dropout: 0.2
    batch_size: ${batch_size}
    dev_every: ${_xnmt_valid_every}
    default_layer_dim: 1000
    restart_trainer: True
    trainer: Adam
    learning_rate: 0.0001
    lr_decay: 0.5
    lr_decay_times: ${anneal_restarts}
    dev_metrics: bleu
    training_corpus: !BilingualTrainingCorpus
      train_src: ${datadir}/train.final.${src}
      train_trg: ${datadir}/train.final.${trg}
      dev_src: ${datadir}/dev.final.${src}
      dev_trg: ${datadir}/dev.final.${trg}
    corpus_parser: !BilingualCorpusParser
      src_reader: !PlainTextReader {}
      trg_reader: !PlainTextReader {}
      max_src_len: ${max_len}
      max_trg_len: ${max_len}
    model: !DefaultTranslator
      src_embedder: !SimpleWordEmbedder
        emb_dim: 512
      encoder: !LSTMEncoder
        layers: 1
        hidden_dim: 1000    # 500 in fact, but with copy-bridge
      attender: !StandardAttender
        hidden_dim: 1000
        state_dim: 1000
        input_dim: 1000
      trg_embedder: !SimpleWordEmbedder
        emb_dim: 512
      decoder: !MlpSoftmaxDecoder
        layers: 1
        mlp_hidden_dim: 500
        #bridge: !NoBridge {}
        bridge: !CopyBridge {}
  decode:
    src_file: ${datadir}/test.final.${src}
    post_process: join-bpe
    beam: ${dev_beam_size}
    #len_norm_type: !PolynomialNormalization # not sure what bug
    #  apply_during_search: True
    #  m: 1.0
  evaluate:
    ref_file: ${datadir}/test.final.${trg}.restore

run:
